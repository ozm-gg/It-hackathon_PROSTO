{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "217795d1-bcc5-4037-9b70-ba9a14632648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import torch.optim as opt\n",
    "import utils\n",
    "import hyp\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import QED\n",
    "from environment import Molecule\n",
    "import replay_buffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from rdkit.Chem import Crippen\n",
    "from rdkit.Chem import Descriptors, QED\n",
    "from rdkit.Contrib.SA_Score import sascorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd1b755-3d75-4c7d-af83-ccfedf889807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "def smiles_to_pdbqt(smiles: str, output_file: str):\n",
    "    \"\"\"Конвертирует SMILES в PDBQT через Open Babel.\"\"\"\n",
    "    # Создание молекулы из SMILES и добавление водородов\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    mol = Chem.AddHs(mol)\n",
    "    AllChem.EmbedMolecule(mol)\n",
    "    \n",
    "    # Сохранение во временный файл .mol\n",
    "    temp_mol = \"temp.mol\"\n",
    "    Chem.MolToMolFile(mol, temp_mol)\n",
    "    \n",
    "    # Конвертация в PDBQT через Open Babel\n",
    "    subprocess.run(f\"obabel {temp_mol} -O {output_file} --gen3d\", shell=True)\n",
    "    os.remove(temp_mol)\n",
    "\n",
    "def run_vina_docking(protein_pdbqt: str, ligand_pdbqt: str, center: tuple, size: tuple = (20, 20, 20)) -> float:\n",
    "    \"\"\"Запускает докинг и возвращает энергию связывания.\"\"\"\n",
    "    # Создание конфигурационного файла для Vina\n",
    "    config = f\"\"\"\n",
    "    receptor = {protein_pdbqt}\n",
    "    ligand = {ligand_pdbqt}\n",
    "    out = result.pdbqt\n",
    "    center_x = {center[0]}\n",
    "    center_y = {center[1]}\n",
    "    center_z = {center[2]}\n",
    "    size_x = {size[0]}\n",
    "    size_y = {size[1]}\n",
    "    size_z = {size[2]}\n",
    "    exhaustiveness = 8\n",
    "    \"\"\"\n",
    "    with open(\"config.txt\", \"w\") as f:\n",
    "        f.write(config)\n",
    "    \n",
    "    # Запуск AutoDock Vina\n",
    "    result = subprocess.run(\n",
    "        \"vina --config config.txt --log log.txt\",\n",
    "        shell=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    # Извлечение энергии связывания из лога\n",
    "    with open(\"log.txt\", \"r\") as f:\n",
    "        log = f.read()\n",
    "    for line in log.split(\"\\n\"):\n",
    "        if \"Affinity\" in line:\n",
    "            return float(line.split()[1])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3575d6cf-961b-4e70-af58-db5cf7bf2b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('error', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d9eb917-ae9f-445b-8e15-2ada11f207ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolDQN(nn.Module):\n",
    "    def __init__(self, input_length, output_length):\n",
    "        super(MolDQN, self).__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(input_length, 1024)\n",
    "        self.linear_2 = nn.Linear(1024, 512)\n",
    "        self.linear_3 = nn.Linear(512, 128)\n",
    "        self.linear_4 = nn.Linear(128, 32)\n",
    "        self.linear_5 = nn.Linear(32, output_length)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.linear_1(x))\n",
    "        x = self.activation(self.linear_2(x))\n",
    "        x = self.activation(self.linear_3(x))\n",
    "        x = self.activation(self.linear_4(x))\n",
    "        x = self.linear_5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d5eb6d5-93fe-48f4-a4a2-f983848dcdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "REPLAY_BUFFER_CAPACITY = hyp.replay_buffer_size\n",
    "\n",
    "irritation_model = load('model_irrit.joblib')\n",
    "melanin_model = load('model_melanin.joblib')\n",
    "\n",
    "def get_fingerprint(molecule):\n",
    "    mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=hyp.fingerprint_radius,fpSize=hyp.fingerprint_length)\n",
    "    if molecule is None:\n",
    "        return np.zeros((hyp.fingerprint_length,))\n",
    "    fingerprint = mfpgen.GetFingerprint(molecule)\n",
    "    return np.array(fingerprint)\n",
    "\n",
    "class QEDRewardMolecule(Molecule):\n",
    "    \n",
    "    def __init__(self, discount_factor, **kwargs):\n",
    "        \n",
    "        super(QEDRewardMolecule, self).__init__(**kwargs)\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def _reward(self):\n",
    "        \n",
    "        molecule = Chem.MolFromSmiles(self._state)\n",
    "        if molecule is None:\n",
    "            return 0.0\n",
    "        qed = QED.qed(molecule)\n",
    "        irrit_proba = irritation_model.predict_proba(np.expand_dims(get_fingerprint(molecule), axis=0))[0, 1]\n",
    "        melanin_proba = melanin_model.predict_proba(np.expand_dims(get_fingerprint(molecule), axis=0))[0, 1]\n",
    "        sa_score = sascorer.calculateScore(molecule)\n",
    "        #-irrit_proba + melanin_proba + permeability_score \n",
    "        return (qed - 0.7 * sa_score - irrit_proba + 0.5 * melanin_model) * self.discount_factor ** (self.num_steps_taken)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e145879-c567-458c-a2c1-19f228d0d792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9959458154732174"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irritation_model.predict_proba(np.expand_dims(get_fingerprint(Chem.MolFromSmiles(\"CC#CC\")), axis=0))[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24537584-e4f3-41a2-988b-929a271dbf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, input_length, output_length, device):\n",
    "        self.device = device\n",
    "        self.dqn, self.target_dqn = (\n",
    "            MolDQN(input_length, output_length).to(self.device),\n",
    "            MolDQN(input_length, output_length).to(self.device),\n",
    "        )\n",
    "        for p in self.target_dqn.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.replay_buffer = replay_buffer.ReplayBuffer(REPLAY_BUFFER_CAPACITY)\n",
    "        self.optimizer = getattr(opt, hyp.optimizer)(\n",
    "            self.dqn.parameters(), lr=hyp.learning_rate\n",
    "        )\n",
    "        self.times_of_update = 0\n",
    "\n",
    "    def get_action(self, observations, epsilon_threshold):\n",
    "\n",
    "        if np.random.uniform() < epsilon_threshold:\n",
    "            action = np.random.randint(0, observations.shape[0])\n",
    "        else:\n",
    "            q_value = self.dqn.forward(observations.to(self.device)).cpu()\n",
    "            action = torch.argmax(q_value).numpy()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update_params(self, batch_size, gamma, polyak):\n",
    "        # update target network\n",
    "\n",
    "        # sample batch of transitions\n",
    "        states, _, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "        q_t = torch.zeros(batch_size, 1, requires_grad=False)\n",
    "        v_tp1 = torch.zeros(batch_size, 1, requires_grad=False)\n",
    "        for i in range(batch_size):\n",
    "            state = (\n",
    "                torch.FloatTensor(states[i])\n",
    "                .reshape(-1, hyp.fingerprint_length + 1)\n",
    "                .to(self.device)\n",
    "            )\n",
    "            q_t[i] = self.dqn(state)\n",
    "\n",
    "            next_state = (\n",
    "                torch.FloatTensor(next_states[i])\n",
    "                .reshape(-1, hyp.fingerprint_length + 1)\n",
    "                .to(self.device)\n",
    "            )\n",
    "            v_tp1[i] = torch.max(self.target_dqn(next_state))\n",
    "\n",
    "        rewards = torch.FloatTensor(rewards).reshape(q_t.shape).to(self.device)\n",
    "        q_t = q_t.to(self.device)\n",
    "        v_tp1 = v_tp1.to(self.device)\n",
    "        dones = torch.FloatTensor(dones).reshape(q_t.shape).to(self.device)\n",
    "\n",
    "        # # get q values\n",
    "        q_tp1_masked = (1 - dones) * v_tp1\n",
    "        q_t_target = rewards + gamma * q_tp1_masked\n",
    "        td_error = q_t - q_t_target\n",
    "\n",
    "        q_loss = torch.where(\n",
    "            torch.abs(td_error) < 1.0,\n",
    "            0.5 * td_error * td_error,\n",
    "            1.0 * (torch.abs(td_error) - 0.5),\n",
    "        )\n",
    "        q_loss = q_loss.mean()\n",
    "\n",
    "        # backpropagate\n",
    "        self.optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.times_of_update % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                for p, p_targ in zip(self.dqn.parameters(), self.target_dqn.parameters()):\n",
    "                    p_targ.data.mul_(polyak)\n",
    "                    p_targ.data.add_((1 - polyak) * p.data)\n",
    "            self.times_of_update += 1\n",
    "        return q_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc540ae-21d6-443f-bdf5-27bc5bbd6b43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward of final molecule at episode 12 is -2.135732820598522, qed is 0.09107870243907015,sa is 6.103670245120666, molecule is CC(N=N)C(O)=C=NNN(C)ON(O)N(N=O)OC=N\n",
      "mean loss in episode 12 is 1.7132644653320312\n",
      "reward of final molecule at episode 14 is -2.126356398780658, qed is 0.2585405335274589,sa is 6.286700660187834, molecule is CNN1NOC2=C(N)C(O)(ONN2)O1\n",
      "mean loss in episode 14 is 0.11283107250928878\n",
      "reward of final molecule at episode 16 is -2.3554398777265635, qed is 0.04217036102538414,sa is 6.658719634376916, molecule is C#CC1C(N=C(O)N=NC(N=N)NN)N1NN(C)NN(N)N\n",
      "mean loss in episode 16 is 0.03935618922114372\n",
      "reward of final molecule at episode 18 is -1.8765669687002655, qed is 0.32837061363055725,sa is 5.673435187372101, molecule is C=C(CC)NN1C(CO)NC(C=O)(N=O)C2=NC21CCO\n",
      "mean loss in episode 18 is 0.00892090166453272\n",
      "reward of final molecule at episode 20 is -1.8829772350842686, qed is 0.09590969158174215,sa is 5.400837104628774, molecule is C=NOC(=O)C(OC(O)CN)N(N)NN\n",
      "mean loss in episode 20 is 0.00450869434280321\n",
      "reward of final molecule at episode 22 is -1.5553198538815687, qed is 0.13347926045826825,sa is 4.528859520521667, molecule is N=C(O)C(=N)OC(O)=NN\n",
      "mean loss in episode 22 is 0.004629497439600528\n",
      "reward of final molecule at episode 24 is -1.877118582002149, qed is 0.13625748012958985,sa is 5.434840811111609, molecule is C#CC1(OC=N)OC(O)(N(C=N)N=O)O1\n",
      "mean loss in episode 24 is 0.002717439504340291\n",
      "reward of final molecule at episode 26 is -0.7942986254405423, qed is 0.33515022311893966,sa is 2.646607471363085, molecule is NC(N)C(=O)O\n",
      "mean loss in episode 26 is 0.002552953129634261\n"
     ]
    }
   ],
   "source": [
    "TENSORBOARD_LOG = True\n",
    "TB_LOG_PATH = \"./runs/dqn/run2\"\n",
    "episodes = 0\n",
    "iterations = 4000\n",
    "update_interval = 2\n",
    "batch_size = 512\n",
    "num_updates_per_it = 1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "environment = QEDRewardMolecule(\n",
    "    discount_factor=hyp.discount_factor,\n",
    "    atom_types=set(hyp.atom_types),\n",
    "    init_mol=hyp.start_molecule,\n",
    "    allow_removal=hyp.allow_removal,\n",
    "    allow_no_modification=hyp.allow_no_modification,\n",
    "    allow_bonds_between_rings=hyp.allow_bonds_between_rings,\n",
    "    allowed_ring_sizes=set(hyp.allowed_ring_sizes),\n",
    "    max_steps=hyp.max_steps_per_episode,\n",
    ")\n",
    "\n",
    "# DQN Inputs and Outputs:\n",
    "# input: appended action (fingerprint_length + 1) .\n",
    "# Output size is (1).\n",
    "\n",
    "agent = Agent(hyp.fingerprint_length + 1, 1, device)\n",
    "\n",
    "if TENSORBOARD_LOG:\n",
    "    writer = SummaryWriter(TB_LOG_PATH)\n",
    "\n",
    "environment.initialize()\n",
    "\n",
    "eps_threshold = 1.0\n",
    "batch_losses = []\n",
    "\n",
    "for it in range(iterations):\n",
    "\n",
    "    steps_left = hyp.max_steps_per_episode - environment.num_steps_taken\n",
    "\n",
    "    valid_actions = list(environment.get_valid_actions())\n",
    "\n",
    "    observations = np.vstack(\n",
    "        [\n",
    "            np.append(\n",
    "                utils.get_fingerprint(\n",
    "                    act, hyp.fingerprint_length, hyp.fingerprint_radius\n",
    "                ),\n",
    "                steps_left,\n",
    "            )\n",
    "            for act in valid_actions\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    observations_tensor = torch.Tensor(observations)\n",
    "\n",
    "    a = agent.get_action(observations_tensor, max(0.1, eps_threshold))\n",
    "\n",
    "   \n",
    "    action = valid_actions[a]\n",
    "    result = environment.step(action)\n",
    "\n",
    "    action_fingerprint = np.append(\n",
    "        utils.get_fingerprint(action, hyp.fingerprint_length, hyp.fingerprint_radius),\n",
    "        steps_left,\n",
    "    )\n",
    "\n",
    "    next_state, reward, done = result\n",
    "\n",
    "    steps_left = hyp.max_steps_per_episode - environment.num_steps_taken\n",
    "\n",
    "    next_state = utils.get_fingerprint(\n",
    "        next_state, hyp.fingerprint_length, hyp.fingerprint_radius\n",
    "    ) \n",
    "\n",
    "    action_fingerprints = np.vstack(\n",
    "        [\n",
    "            np.append(\n",
    "                utils.get_fingerprint(\n",
    "                    act, hyp.fingerprint_length, hyp.fingerprint_radius\n",
    "                ),\n",
    "                steps_left,\n",
    "            )\n",
    "            for act in environment.get_valid_actions()\n",
    "        ]\n",
    "    )  \n",
    "\n",
    "\n",
    "    agent.replay_buffer.add(\n",
    "        obs_t=action_fingerprint,  # (fingerprint_length + 1)\n",
    "        action=0,  # No use\n",
    "        reward=reward,\n",
    "        obs_tp1=action_fingerprints,  # (num_actions, fingerprint_length + 1)\n",
    "        done=float(result.terminated),\n",
    "    )\n",
    "\n",
    "    if done:\n",
    "        final_reward = reward\n",
    "        if episodes != 0 and TENSORBOARD_LOG and len(batch_losses) != 0:\n",
    "            writer.add_scalar(\"episode_reward\", final_reward, episodes)\n",
    "            writer.add_scalar(\"episode_loss\", np.array(batch_losses).mean(), episodes)\n",
    "        if episodes != 0 and episodes % 2 == 0 and len(batch_losses) != 0:\n",
    "            print(\n",
    "                \"reward of final molecule at episode {} is {}, qed is {},sa is {}, molecule is {}\".format(\n",
    "                    episodes, final_reward, QED.qed(Chem.MolFromSmiles(environment._state)), sascorer.calculateScore(Chem.MolFromSmiles(environment._state)),  environment._state\n",
    "                )\n",
    "            )\n",
    "            print(\n",
    "                \"mean loss in episode {} is {}\".format(\n",
    "                    episodes, np.array(batch_losses).mean()\n",
    "                )\n",
    "            )\n",
    "        episodes += 1\n",
    "        eps_threshold -= 0.01\n",
    "        batch_losses = []\n",
    "        environment.initialize()\n",
    "\n",
    "    if it % update_interval == 0 and agent.replay_buffer.__len__() >= batch_size:\n",
    "        for update in range(num_updates_per_it):\n",
    "            loss = agent.update_params(batch_size, hyp.gamma, hyp.polyak)\n",
    "            loss = loss.item()\n",
    "            batch_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9071e5-6b23-43a6-b35c-089a43e1d112",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generated_molecules = []\n",
    "num_molecules_to_generate = 10\n",
    "agent.dqn.eval()\n",
    "eps_threshold = 0.03\n",
    "\n",
    "for it in range(num_molecules_to_generate):\n",
    "    done = False\n",
    "    environment.initialize()\n",
    "    while not done:\n",
    "        steps_left = hyp.max_steps_per_episode - environment.num_steps_taken\n",
    "        valid_actions = list(environment.get_valid_actions())\n",
    "    \n",
    "        observations = np.vstack(\n",
    "            [\n",
    "                np.append(\n",
    "                    utils.get_fingerprint(\n",
    "                        act, hyp.fingerprint_length, hyp.fingerprint_radius\n",
    "                    ),\n",
    "                    steps_left,\n",
    "                )\n",
    "                for act in valid_actions\n",
    "            ]\n",
    "        ) \n",
    "    \n",
    "        observations_tensor = torch.Tensor(observations)\n",
    "        a = agent.get_action(observations_tensor, eps_threshold)\n",
    "        action = valid_actions[a]\n",
    "        result = environment.step(action)\n",
    "    \n",
    "        action_fingerprint = np.append(\n",
    "            utils.get_fingerprint(action, hyp.fingerprint_length, hyp.fingerprint_radius),\n",
    "            steps_left,\n",
    "        )\n",
    "    \n",
    "        next_state, reward, done = result\n",
    "        steps_left = hyp.max_steps_per_episode - environment.num_steps_taken\n",
    "    \n",
    "        next_state = utils.get_fingerprint(\n",
    "            next_state, hyp.fingerprint_length, hyp.fingerprint_radius\n",
    "        )  \n",
    "    \n",
    "        action_fingerprints = np.vstack(\n",
    "            [\n",
    "                np.append(\n",
    "                    utils.get_fingerprint(\n",
    "                        act, hyp.fingerprint_length, hyp.fingerprint_radius\n",
    "                    ),\n",
    "                    steps_left,\n",
    "                )\n",
    "                for act in environment.get_valid_actions()\n",
    "            ]\n",
    "        )\n",
    "        #print(environment._state)\n",
    "    \n",
    "    \n",
    "    generated_molecules.append(environment._state)\n",
    "    print(generated_molecules[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfcda02-d503-45c1-9cd8-94caa19f7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e69c97-e936-4e99-80c6-184ec4bc62e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "valid_smiles = []\n",
    "for smi in generated_molecules:\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is not None:\n",
    "        valid_smiles.append(smi)\n",
    "\n",
    "print(f\"Сгенерировано валидных молекул: {len(valid_smiles)}/{len(generated_molecules)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca3779-f1b3-4320-8222-4fde47106505",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Пример сгенерированных молекул:\")\n",
    "for smi in valid_smiles[:5]:\n",
    "    print(smi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ea590-7b7c-483b-aa36-f9c9f0f22fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
